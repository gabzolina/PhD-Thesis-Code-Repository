{"cells":[{"cell_type":"code","execution_count":null,"id":"3462d7e7","metadata":{"id":"3462d7e7"},"outputs":[],"source":["import pandas as pd #Pandas handles tabular data\n","pd.set_option('display.float_format', lambda x: '%.3f' % x) # turn off scientific notation and too much decimal blah\n","import matplotlib.pyplot as plt # standard plotting library\n","import numpy as np #Numpy for linear algebra & co\n","import seaborn as sns # For pretty dataviz\n","sns.set_style(\"whitegrid\") # Define style for dataviz\n","import pickle\n","from scipy.stats import truncnorm\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n"]},{"cell_type":"code","execution_count":null,"id":"4482926b","metadata":{"tags":[],"id":"4482926b"},"outputs":[],"source":["# Load data\n","\n","data = pd.read_csv('DemonstratorDataset.csv', delim_whitespace= True)\n","pd.set_option('display.max_columns', None)\n","\n","#print(data)"]},{"cell_type":"markdown","id":"322ef07a","metadata":{"tags":[],"id":"322ef07a"},"source":["## Statistical Analaysis"]},{"cell_type":"code","execution_count":null,"id":"1b6b25e6","metadata":{"id":"1b6b25e6"},"outputs":[],"source":["data.describe()"]},{"cell_type":"code","execution_count":null,"id":"e66ad1c6","metadata":{"id":"e66ad1c6"},"outputs":[],"source":["data.hist(figsize=(20, 15),color=\"firebrick\")\n","plt.savefig('dataHist.jpg')"]},{"cell_type":"code","execution_count":null,"id":"6e03bffd","metadata":{"id":"6e03bffd"},"outputs":[],"source":["#Create BlockID column\n","data['pos'] = data['edgeID'].str.find(';')\n","data['BlockID'] = data.apply(lambda x: x['edgeID'][0:x['pos']],axis=1)\n","data[\"BlockID\"] = data[\"BlockID\"].str[1:]\n","data[\"WetLength\"] = data[\"WetLength\"] * 10\n","data[\"DryLength\"] = data[\"DryLength\"] * 10\n","\n","data = data.drop('pos', 1)\n","print(data)"]},{"cell_type":"markdown","id":"88f2f2b4","metadata":{"id":"88f2f2b4"},"source":["## Correlation analysis"]},{"cell_type":"code","execution_count":null,"id":"3ba3356a","metadata":{"id":"3ba3356a"},"outputs":[],"source":["data_num = data[[\"Offset\",\"PrintHeight\", \"WetArea\", \"WetPerim\", \"DeltaHeight\", \"DeltaArea\", \"PlateAngY\", \"PlateAngX\", \"Humidity\", \"Temperature\", \"WetLength\", \"WetAngStart\" ]]"]},{"cell_type":"code","execution_count":null,"id":"ee5c4d22","metadata":{"id":"ee5c4d22"},"outputs":[],"source":["#correlation matrix\n","plt.figure(figsize=(12, 6))\n","corr = sns.heatmap(data_num.corr(), cmap = \"rocket\", annot=True, vmin = -1, vmax = 1)\n","corr.set_title('Numerical Value Correlation Heatmap')\n","plt.savefig('correlation_matrix.jpg')"]},{"cell_type":"markdown","id":"ecd29f73","metadata":{"id":"ecd29f73"},"source":["## Delta Height as variable"]},{"cell_type":"code","execution_count":null,"id":"a05802ae","metadata":{"id":"a05802ae"},"outputs":[],"source":["#https://medium.com/@morganjonesartist/color-guide-to-seaborn-palettes-da849406d44f\n","sns.set_style(\"whitegrid\")\n","from matplotlib.patches import Patch\n","from matplotlib.lines import Line2D\n","from matplotlib.offsetbox import OffsetImage,AnnotationBbox\n","\n","#dic for color mapping\n","di = {0 : 'w', 1: 'r', 2: 'b'}\n","\n","legend_elements = [Patch(facecolor='w', edgecolor='w', label='None'),\n","                   Patch(facecolor='w', edgecolor='r', label='Male'),\n","                   Patch(facecolor='w', edgecolor='b', label='Female')]\n","\n","#Add block thumbnail under xtick\n","#https://stackoverflow.com/questions/44246650/add-image-annotations-to-bar-plots\n","\n","def get_thumb(name):\n","     path = \"./thumb/{}.png\".format(name)\n","     im = plt.imread(path)\n","     return im\n","\n","\n","def offset_image(coord, name, ax):\n","    img = get_thumb(name)\n","    im = OffsetImage(img, zoom=0.5)\n","    im.image.axes = ax\n","\n","    ab = AnnotationBbox(im, (coord, 20),  xybox=(0., -50), frameon=False,\n","                        xycoords='data',  boxcoords=\"offset points\", pad=0)\n","\n","    ax.add_artist(ab)"]},{"cell_type":"code","execution_count":null,"id":"d1660e27","metadata":{"id":"d1660e27"},"outputs":[],"source":["f = sns.relplot(x=\"BlockID\", y=\"DeltaHeight\", hue = \"EdgesNum\", style = \"Cross?\", edgecolor = (data[\"FemMal?\"].map(di)), palette = \"Dark2\", data = data, height = 12, s=150, linewidth = 1)\n","f.set_ylabels(\"Delta Height (mm)\", clear_inner=False)\n","f.set_xlabels(\" \", clear_inner=False)\n","leg = f._legend\n","leg.set_bbox_to_anchor([1, 0.6])\n","\n","blocks = list(data.BlockID.unique())\n","\n","for axes in f.axes.flat:\n","    axes.set_xticklabels(axes.get_xticklabels(), rotation=0)\n","    axes.set_title(\"Height Shrinkage\")\n","    axes.set_yticks(np.arange(20,45,1))\n","    axes.grid(True, axis = \"both\")\n","    for i, c in enumerate(blocks):\n","        offset_image(i, c, axes)\n","    axes.legend(handles=legend_elements,frameon=False, loc='best', bbox_to_anchor=(0.6, -0.035, 0.5, 0.5), title = \"Joint\")\n","\n","f.savefig(\"Height Shrinkage.jpg\")"]},{"cell_type":"code","execution_count":null,"id":"cfd7d20e","metadata":{"id":"cfd7d20e"},"outputs":[],"source":["pp = sns.pairplot(data = data, x_vars= [\"PrintHeight\", \"Offset\", \"WetLength\"],\n","                  y_vars = [\"DeltaHeight\"],\n","                  hue = \"FemMal?\", height = 5)\n","pp.fig.suptitle(\"Geometric feature correlation\")\n","pp.savefig(\"Corr.jpg\")"]},{"cell_type":"code","execution_count":null,"id":"985e4d31","metadata":{"tags":[],"id":"985e4d31"},"outputs":[],"source":["ppp = sns.pairplot(data = data, x_vars= [\"WetArea\", \"WetPerim\", \"WetAngStart\"],\n","                  y_vars = [\"DeltaHeight\"],\n","                  hue = \"FemMal?\", height = 5)\n","ppp.fig.suptitle(\"Geometric feature correlation\")\n","ppp.savefig(\"Corr2.jpg\")"]},{"cell_type":"markdown","id":"f280f389","metadata":{"id":"f280f389"},"source":["## CLUSTER DATA FOR TRAIN TEST SPLIT"]},{"cell_type":"code","execution_count":null,"id":"f28b3cf0","metadata":{"id":"f28b3cf0"},"outputs":[],"source":["print(data.columns)"]},{"cell_type":"code","execution_count":null,"id":"458e3339","metadata":{"id":"458e3339"},"outputs":[],"source":["#extract numerical data\n","data_num = data[['Offset', 'EdgesNum', 'PrintHeight',\n","       'WetLength', 'WetAngStart', 'WetAngEnd', 'WetArea', 'WetPerim',\n","                 'PlateAngY', 'PlateAngX', 'Humidity', 'Temperature' ]]\n","print(data_num.shape)\n","\n","#scale for PCA\n","from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler()\n","data_num_scaled = scaler.fit_transform(data_num)\n","\n","#Train PCA\n","from sklearn.decomposition import PCA\n","model_pca = PCA()\n","model_pca.fit(data_num_scaled)\n","data_num_reduced = model_pca.transform(data_num_scaled)\n","data_num_reduced_df = pd.DataFrame(data_num_reduced)\n","data_cat = data[['FemMal?', 'Cross?', 'BlockID']]\n","data_num_reduced_df= data_num_reduced_df.join(data_cat)\n","data_num_reduced_df[\"DeltaHeight\"] = data[\"DeltaHeight\"]\n","\n","#plot PCA components\n","# Plot the explained variances\n","features = range(model_pca.n_components_)\n","a = plt.bar(features, model_pca.explained_variance_ratio_, color='black')\n","plt.xlabel('PCA features')\n","plt.ylabel('variance %')\n","plt.xticks(features)\n","plt.savefig(\"PCAfeatures.jpg\")"]},{"cell_type":"code","execution_count":null,"id":"0bf89cb6","metadata":{"id":"0bf89cb6"},"outputs":[],"source":["def myplot_df(df,coeff, scale, labels = None):\n","    df[\"xs\"] = df[df.columns[0]]\n","    df[\"ys\"] = df[df.columns[1]]\n","    #print(df)\n","\n","    n = coeff.shape[0]\n","\n","\n","    if scale == True:\n","        scalex = 1.0/(df[\"xs\"].max() - df[\"xs\"].min())\n","        scaley = 1.0/(df[\"ys\"].max() - df[\"ys\"].min())\n","\n","        df[\"xs\"] = scalex *  df[\"xs\"]\n","        df[\"ys\"] = scaley * df[\"ys\"]\n","\n","    sns.set_style(\"whitegrid\")\n","    sns.set(rc={'axes.facecolor':'#f0f0f0', 'figure.facecolor':'#f0f0f0'})\n","    #sns.set(rc={'axes.facecolor':'#ffffff', 'figure.facecolor':'#ffffff'})\n","\n","    g = sns.scatterplot(x='xs',y='ys', data=df, s=100,\n","                        hue = \"BlockID\", style = \"Cross?\", edgecolor = (data[\"FemMal?\"].map(di)), linewidth = 2, alpha = 0.9, palette = \"magma\")\n","    g.legend(title = \"BlockID\", loc='center left', bbox_to_anchor=(1, 0.6), ncol=1, frameon = False)\n","\n","    #name of columns is hardcoded, pay attention\n","    for i in range(n):\n","        plt.arrow(0, 0, coeff[i,0], coeff[i,1],color = 'r',alpha = 0.5)\n","        if labels is None:\n","            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, data_num.columns[i], color = 'g', ha = 'center', va = 'center')\n","        else:\n","            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, labels[i], color = 'g', ha = 'center', va = 'center')\n","\n","\n","    plt.savefig(\"PCA_plot.jpg\")"]},{"cell_type":"code","execution_count":null,"id":"9f8e2f53","metadata":{"id":"9f8e2f53"},"outputs":[],"source":["from matplotlib.patches import Patch\n","from matplotlib.lines import Line2D\n","from matplotlib.offsetbox import OffsetImage,AnnotationBbox\n","\n","#dic for color mapping\n","di = {0 : 'w', 1: 'r', 2: 'b'}\n","\n","legend_elements = [Patch(facecolor='w', edgecolor='w', label='None'),\n","                   Patch(facecolor='w', edgecolor='r', label='Male'),\n","                   Patch(facecolor='w', edgecolor='b', label='Female')]\n","\n","\n","plt.xlim(-1,1)\n","plt.ylim(-1,1)\n","plt.xlabel(\"PC{}\".format(1))\n","plt.ylabel(\"PC{}\".format(2))\n","fig = plt.gcf()\n","fig.set_size_inches(12, 10)\n","fig.legend(handles=legend_elements,frameon=False, bbox_to_anchor=(0.51, -0.21, 0.5, 0.5), title = \"Joint\")\n","myplot_df(data_num_reduced_df, np.transpose(model_pca.components_[0:2, :]), True)"]},{"cell_type":"code","execution_count":null,"id":"704c1eb5","metadata":{"id":"704c1eb5"},"outputs":[],"source":["#helper function\n","from sklearn.cluster import KMeans\n","\n","def findClustNumb(rng, data, x):\n","    #rng: range(1,x) x being max numb of clusters to test\n","    #data: df reduced data components\n","    #x: how many columns/components to take into account\n","\n","    ks = rng\n","    inertias = []\n","\n","    for k in ks:\n","        # Create a KMeans instance with k clusters: model\n","        model = KMeans(n_clusters=k)\n","\n","        # Fit model to samples\n","        model.fit(data.iloc[:,:x])\n","\n","        # Append the inertia to the list of inertias\n","        inertias.append(model.inertia_)\n","    plt.figure(figsize = (6, 6))\n","    plt.plot(ks, inertias, '-o', color='black')\n","    plt.xlabel('number of clusters, k')\n","    plt.ylabel('inertia')\n","    plt.xticks(ks)\n","    plt.savefig(\"inertiaElbow.jpg\")\n","    plt.show()\n","\n","\n","def clusterPlot(n, reduced_data, x, i, show):\n","\n","    #n: number of kmeans clusters\n","    #data: df reduced data components\n","    #x: how many columns/components to take into account\n","    #i : tag for legend\n","\n","    #run kmeans\n","    kmeans = KMeans(n_clusters=n, random_state= 42)\n","    kmeans.fit(reduced_data.iloc[:,:x])\n","\n","    reduced_data[\"cluster\"] = kmeans.predict(reduced_data.iloc[:,:x])\n","\n","    centroids = kmeans.cluster_centers_\n","\n","\n","    dim0_min, dim0_max= reduced_data.iloc[:, 0].min() - 1, reduced_data.iloc[:, 0].max() + 1\n","    dim1_min, dim1_max= reduced_data.iloc[:, 1].min() - 1, reduced_data.iloc[:, 1].max() + 1\n","\n","    sns.set(rc={'axes.facecolor':'#ffffff', 'figure.facecolor':'#ffffff'})\n","\n","    if show == True:\n","        g=sns.scatterplot(x=0, y=1,data = reduced_data,  hue = \"cluster\", cmap = \"BrBG\", s=100, linewidth = 1)\n","        sns.scatterplot(x = centroids[:, 0], y =centroids[:, 1], marker = \"X\", s = 200, color = \"r\", zorder = 10).set(title = \"K-means clustering on %s data\" % i)\n","        g.legend(title = \"cluster\", loc='right', bbox_to_anchor=(1.25, 0.5), ncol=1)\n","        fig = plt.gcf()\n","        fig.set_size_inches(10, 10)\n","        fig.savefig(\"kmeansPCA.jpg\")\n","    else:\n","        pass\n","\n","    return kmeans, reduced_data"]},{"cell_type":"code","execution_count":null,"id":"7450e8a9","metadata":{"id":"7450e8a9"},"outputs":[],"source":["findClustNumb(range(1,10), data_num_reduced_df, 6)"]},{"cell_type":"code","execution_count":null,"id":"a83db6b2","metadata":{"id":"a83db6b2"},"outputs":[],"source":["kmeans_pca, clustered_PCAdata = clusterPlot(8, data_num_reduced_df, 6, \"PCA-reduced\", True)"]},{"cell_type":"code","execution_count":null,"id":"e901e575","metadata":{"id":"e901e575"},"outputs":[],"source":["#bring clusters to dataframe\n","data_num[\"clusterPCA\"] = clustered_PCAdata[\"cluster\"]\n","data[\"clusterPCA\"] = clustered_PCAdata[\"cluster\"]\n","\n","a = sns.catplot(kind = \"count\", x = \"clusterPCA\", data = data_num, palette = \"rocket\")\n","#a.savefig(\"clusterCount.jpg\")"]},{"cell_type":"markdown","id":"8ff88b01","metadata":{"id":"8ff88b01"},"source":["Viz"]},{"cell_type":"code","execution_count":null,"id":"ab3d6b1d","metadata":{"id":"ab3d6b1d"},"outputs":[],"source":["b = sns.catplot(col = \"clusterPCA\", x = \"FemMal?\", data = data, kind = \"count\", height = 3, hue = \"FemMal?\")"]},{"cell_type":"code","execution_count":null,"id":"65dd0483","metadata":{"id":"65dd0483"},"outputs":[],"source":["b = sns.catplot(col = \"clusterPCA\", x = \"Cross?\", data = data, kind = \"count\", height = 3, hue = \"Cross?\")"]},{"cell_type":"code","execution_count":null,"id":"659bff76","metadata":{"id":"659bff76"},"outputs":[],"source":["b = sns.displot(col = \"clusterPCA\", x = \"WetLength\", data = data,hue = \"BlockID\", multiple = \"stack\")"]},{"cell_type":"markdown","id":"806856bb","metadata":{"id":"806856bb"},"source":["## Prepare data for augmentation"]},{"cell_type":"code","execution_count":null,"id":"285d95da","metadata":{"id":"285d95da"},"outputs":[],"source":["##########PREPARE DATA\n","\n","#normal dataset\n","data_p= data_num.join(data_cat)\n","#make joint type dummy data and rename columns\n","dummies = pd.get_dummies(data = data_p, columns = [\"FemMal?\"], drop_first = True)\n","colnames = dummies.columns.values.tolist()\n","colnames = colnames[:-2] + [\"maleJ\", \"femaleJ\"]\n","dummies.columns = colnames\n","dummies[\"DeltaHeight\"] = data[\"DeltaHeight\"]\n","\n","print(dummies.shape)\n","#dummies now has the numerical values that need to be standard scaled\n","\n","\n","#reduced dataset\n","data_num_reduced_df = data_num_reduced_df.iloc[:, : 6]\n","data_num_reduced_df[\"DeltaHeight\"] = data[\"DeltaHeight\"]\n","print(data_num_reduced_df.shape)"]},{"cell_type":"code","execution_count":null,"id":"de21d1d6","metadata":{"id":"de21d1d6"},"outputs":[],"source":["############SPLIT TRAIN AND TEST\n","spg = max(2, int(min(list(data.groupby(\"clusterPCA\").size())) / 8))\n","data_test_idx = dummies.groupby(\"clusterPCA\").sample(n=spg, random_state=40).index   # IMPORTANT SET RANDOOM STATE\n","print(\" %s test samples\" % len(data_test_idx), data_test_idx)"]},{"cell_type":"code","execution_count":null,"id":"3101a8ea","metadata":{"id":"3101a8ea"},"outputs":[],"source":["data"]},{"cell_type":"code","execution_count":null,"id":"f43d5a60","metadata":{"id":"f43d5a60"},"outputs":[],"source":["#Visualize TrainTest split\n","data_v = pd.read_csv('DemonstratorDataset.csv', delim_whitespace= True)\n","data_v['pos'] = data_v['edgeID'].str.find(';')\n","data_v['BlockID'] = data_v.apply(lambda x: x['edgeID'][0:x['pos']],axis=1)\n","data_v[\"BlockID\"] = data_v[\"BlockID\"].str[1:]\n","data_v[\"WetLength\"] = data_v[\"WetLength\"] * 10\n","data_v[\"DryLength\"] = data_v[\"DryLength\"] * 10\n","data_v = data_v.drop('pos', 1)\n","\n","\n","d_test = data_v.iloc[data_test_idx]\n","d_train = data_v.drop(data_test_idx)\n","\n","fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n","\n","h = sns.histplot(ax = axes[0,0], data=d_train, x=\"DeltaHeight\", hue=\"FemMal?\", multiple=\"stack\")\n","g = sns.histplot(ax = axes[1,0], data=d_train, x=\"DeltaHeight\", hue=\"Cross?\", multiple=\"stack\")\n","j = sns.histplot(ax = axes[0,1], data=d_test, x=\"DeltaHeight\", hue=\"FemMal?\", multiple=\"stack\")\n","k = sns.histplot(ax = axes[1,1], data=d_test, x=\"DeltaHeight\", hue=\"Cross?\", multiple=\"stack\")\n","fig.savefig(\"trainTest.jpg\")"]},{"cell_type":"code","execution_count":null,"id":"68bc6c85","metadata":{"id":"68bc6c85"},"outputs":[],"source":["#augmentation instructions\n","instructionsDic = {\"Offset\" : [0, 1, -2, +2],\n","                   \"PrintHeight\": [0,1, -2, 2],\n","                   \"WetLength\" : [0,0.5, -1, 1],\n","                   \"WetAngStart\" : [0, 0.125, -0.25, 0.25],\n","                   \"WetArea\" : [0, 0.0005, -0.001, 0.001],\n","                   \"DeltaHeight\" : [0,0.5,-1,1]}\n","catCols = [\"EdgesNum\",\"clusterPCA\",\"Cross?\", \"BlockID\", \"maleJ\", \"femaleJ\"]\n","times = 5"]},{"cell_type":"code","execution_count":null,"id":"8e1ca8d2","metadata":{"id":"8e1ca8d2"},"outputs":[],"source":["def createDuplicate(df, colnames, times):\n","    dfCat = df[colnames]\n","    for i in range(times):\n","        dfCat = dfCat.append(df[colnames], ignore_index=True)\n","        #print(\"round %s, the df now has %s entries\" % (i, len(df.index)))\n","\n","    return dfCat"]},{"cell_type":"code","execution_count":null,"id":"9a820c7e","metadata":{"id":"9a820c7e"},"outputs":[],"source":["def createNoise(df, colname, mean, sd, low, upp, times):\n","    #init\n","    original = df[colname].dropna()\n","    #print(original)\n","    rowsNum = df.shape[0]\n","    #print(\"Working on column %s, currently %s rows\" % (colname, rowsNum))\n","    plt.figure(colname)\n","\n","    #make noisyDF\n","    for i in range(times):\n","        #make noise array using truncated normal distribution\n","        np.random.seed(seed=i)\n","        noiseSp = truncnorm((low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd)\n","        noiseArr = np.asarray(noiseSp.rvs(rowsNum))\n","        #print(noiseArr)\n","        #sns.histplot(noiseArr).set_title(\"Noise for %s\" % colname)\n","        #print(pd.DataFrame(noiseArr).describe())\n","\n","        #Create a copy of the column values + noise\n","        noisyVal = np.add(noiseArr, original)\n","        #print(noisyVal)\n","\n","        df = df.append(pd.DataFrame(noisyVal, columns=[colname]), ignore_index=True)\n","        #print(\"round %s, the column now has %s entries\" % (i, df[colname].count()))\n","\n","    return df[colname]"]},{"cell_type":"code","execution_count":null,"id":"e810c8b6","metadata":{"id":"e810c8b6"},"outputs":[],"source":["###########augment data\n","#get data train\n","data_train = dummies.drop(data_test_idx).reset_index(drop = True)\n","print(data_train)\n","\n","if times > 1:\n","\n","    #########CREATE AUGMENTED DATA\n","    #init\n","    noisy = pd.DataFrame()\n","    #print(noisy)\n","\n","    #Numerical values\n","    for key, value in instructionsDic.items():\n","        noisy[key] = createNoise(data_train, key, value[0], value[1], value[2], value[3], times)\n","\n","    #CategoricalValues\n","    duplicateCats = createDuplicate(data_train, catCols,times)\n","\n","    #Merge\n","    augmented = noisy.join(duplicateCats)\n","    augmented = augmented[:-1]\n","\n","    #create new column to see if it is real or synthetic\n","    augmented[\"origin\"] = \"real\"\n","    augmented[\"origin\"][augmented.index > data_train.shape[0] ] = \"synthetic\"\n","\n","\n","    data = augmented\n","\n","else:\n","    data[\"origin\"] = \"real\"\n","\n","\n","print(augmented.info())\n","print(augmented)"]},{"cell_type":"code","execution_count":null,"id":"affb1778","metadata":{"id":"affb1778"},"outputs":[],"source":["#############VIZUALIZE AUGMENTED DATA\n","for key, value in instructionsDic.items():\n","    print(key)\n","    df_real = augmented[augmented['origin'] == 'real'][key].reset_index(drop=True)\n","    df_synth = augmented[augmented['origin'] == 'synthetic'][key].reset_index(drop=True)\n","    new_df = pd.concat([df_real, df_synth], axis=1)\n","    new_df.columns = [\"real\", \"synthetic\"]\n","\n","    plt.figure(key)\n","    print(new_df.describe().transpose())\n","    a = sns.kdeplot(data = new_df, palette = \"PuRd\", common_norm = False, legend = True).set(title = key)\n","    plt.gcf().set_size_inches(5, 5)\n","    plt.savefig(\"a%s.png\" % key)\n","    plt.show()\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"bbb679c3","metadata":{"id":"bbb679c3"},"outputs":[],"source":["################## PROJECT AUGMENTED INTO PCA"]},{"cell_type":"code","execution_count":null,"id":"de30299b","metadata":{"id":"de30299b"},"outputs":[],"source":["print(augmented.columns)\n","aug_cat = augmented[['origin']]"]},{"cell_type":"code","execution_count":null,"id":"3668f1e8","metadata":{"id":"3668f1e8"},"outputs":[],"source":["augmented_num = augmented[['Offset', 'PrintHeight', 'WetLength', 'WetAngStart', 'WetArea','DeltaHeight']]\n","from sklearn.preprocessing import StandardScaler\n","scalerAug = StandardScaler()\n","aug_num_scaled = scalerAug.fit_transform(augmented_num)\n","\n","from sklearn.decomposition import PCA\n","model_pca_aug = PCA()\n","model_pca_aug.fit(aug_num_scaled)\n","aug_num_reduced = model_pca_aug.transform(aug_num_scaled)\n","\n","aug_num_reduced_df = pd.DataFrame(aug_num_reduced)\n","aug_num_reduced_df= aug_num_reduced_df.join(aug_cat)\n","aug_num_reduced_df\n","\n"]},{"cell_type":"code","execution_count":null,"id":"805570a5","metadata":{"id":"805570a5"},"outputs":[],"source":["def myplot_df_aug(df,coeff, scale, labels = None):\n","    df[\"xs\"] = df[df.columns[0]]\n","    df[\"ys\"] = df[df.columns[1]]\n","    #print(df)\n","\n","    n = coeff.shape[0]\n","\n","\n","    if scale == True:\n","        scalex = 1.0/(df[\"xs\"].max() - df[\"xs\"].min())\n","        scaley = 1.0/(df[\"ys\"].max() - df[\"ys\"].min())\n","\n","        df[\"xs\"] = scalex *  df[\"xs\"]\n","        df[\"ys\"] = scaley * df[\"ys\"]\n","\n","#     g = sns.scatterplot(x='xs',y='ys', data=df, s=100,\n","#                         hue = \"origin\", style = \"Cross?\", edgecolor = (data[\"FemMal?\"].map(di)), linewidth = 1, alpha = 0.9)\n","    g = sns.scatterplot(x='xs',y='ys', data=df, s= df[\"origin\"].map({\"real\" : 200, \"synthetic\" : 50}),\n","                        hue = \"origin\", linewidth = 1, alpha = 0.5)\n","    g.legend(title = \"BlockID\", loc='center left', bbox_to_anchor=(1, 0.5), ncol=1, frameon = False)\n","\n","    for i in range(n):\n","        plt.arrow(0, 0, coeff[i,0], coeff[i,1],color = 'r',alpha = 0.5)\n","        if labels is None:\n","            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, augmented_num.columns[i], color = 'g', ha = 'center', va = 'center', alpha = 0.5)\n","        else:\n","            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, labels[i], color = 'g', ha = 'center', va = 'center')\n","\n","plt.xlim(-0.5,0.7)\n","plt.ylim(-0.5,0.7)\n","plt.xlabel(\"PC{}\".format(1))\n","plt.ylabel(\"PC{}\".format(2))\n","fig = plt.gcf()\n","fig.set_size_inches(12, 12)\n","myplot_df_aug(aug_num_reduced_df, np.transpose(model_pca_aug.components_[0:2, :]), True)\n","fig.savefig(\"augmentedPCA.jpg\")\n"]},{"cell_type":"markdown","id":"e7ead841","metadata":{"id":"e7ead841"},"source":["## prepare data for training"]},{"cell_type":"code","execution_count":null,"id":"5bfcf8fe","metadata":{"id":"5bfcf8fe"},"outputs":[],"source":["dummies.info()\n","#keep all 15 features\n","dummies = dummies.drop(columns = [\"clusterPCA\", \"BlockID\"])\n","dummies"]},{"cell_type":"markdown","id":"f977f1ce","metadata":{"id":"f977f1ce"},"source":["## MODEL TRAINING"]},{"cell_type":"markdown","id":"57f67f84","metadata":{"id":"57f67f84"},"source":["### 15-Feature data"]},{"cell_type":"code","execution_count":null,"id":"bf8e3070","metadata":{"id":"bf8e3070"},"outputs":[],"source":["def simplePredict(dummies,data_test_idx, model, tag):\n","\n","    dummies_x = dummies.drop(columns = [\"DeltaHeight\"])\n","\n","    from sklearn.preprocessing import StandardScaler\n","    scaler = StandardScaler()\n","    dummies_scaled = scaler.fit_transform(dummies_x)\n","\n","    filename = 'stdScaler_15features.pk'\n","    pickle.dump(scaler, open(filename, 'wb'))\n","\n","    dummies_scaled_df = pd.DataFrame(dummies_scaled, columns=dummies_x.columns)\n","    dummies_scaled_df[\"DeltaHeight\"] = dummies[\"DeltaHeight\"]\n","\n","    data_test = dummies_scaled_df.iloc[data_test_idx]\n","    data_train = dummies_scaled_df.drop(data_test_idx)\n","\n","    x_train_scaled = data_train.drop(columns = [\"DeltaHeight\"])\n","    y_train = data_train[[\"DeltaHeight\"]]\n","\n","    x_test_scaled = data_test.drop(columns = [\"DeltaHeight\"])\n","    y_test = data_test[[\"DeltaHeight\"]]\n","\n","\n","    print(x_train_scaled.shape)\n","    print(y_train.shape)\n","\n","    print(x_test_scaled.shape)\n","    print(y_test.shape)\n","\n","\n","    #TRAIN MODEL\n","    from sklearn.metrics import mean_absolute_error\n","    np.random.seed(2)\n","\n","    print(\"now training \" + key )\n","    if key == \"Artificial Neural Network\":\n","        history = model.fit(x_train_scaled,y_train,epochs=400, validation_split=0.1, verbose = 0)\n","\n","    else:\n","\n","\n","        model.fit(x_train_scaled,y_train)\n","\n","    y_pred_train = model.predict(x_train_scaled)\n","    y_pred_test = model.predict(x_test_scaled)\n","\n","    mae_train = round(mean_absolute_error(y_train, y_pred_train), 2)\n","    mae_test = round(mean_absolute_error(y_test, y_pred_test), 2)\n","\n","    fig,ax = plt.subplots(figsize = (5,5))\n","    plt.suptitle (\"%s \\n mae train =  %smm \\n mae test = %smm  \" % (tag, mae_train,mae_test), y=1)\n","\n","    ax.plot([0, 1], [0, 1], transform=ax.transAxes, linewidth=1, color='black', ls = \":\", alpha = 0.5)\n","    l1 = ax.scatter(y_pred_train, y_train, c= \"m\", alpha = 0.6, label = \"train\")\n","    l2= ax.scatter(y_pred_test, y_test, c= \"r\", alpha = 0.7, label = \"test\")\n","    ax.legend(handles = [l1, l2])\n","    ax.set_xlabel(\"y prediction (mm)\")\n","    ax.set_ylabel(\"y truth (mm) \")\n","    plt.ylim(20, 45)\n","    plt.xlim(20,45)\n","    plt.savefig(\"rawData%s.png\" % key)\n","\n","\n","    return model, mae_train, mae_test, y_pred_train, y_pred_test\n"]},{"cell_type":"code","execution_count":null,"id":"35f5ebdb","metadata":{"id":"35f5ebdb"},"outputs":[],"source":["from sklearn.linear_model import LinearRegression\n","model_lr = LinearRegression()\n","\n","from sklearn.kernel_ridge import KernelRidge\n","model_krr = KernelRidge(alpha=1.0,kernel='polynomial',degree=5)\n","\n","import xgboost as xgb\n","model_xgb = xgb.XGBRegressor(max_depth = 4, n_estimators = 20)\n","\n","import tensorflow as tf\n","tf.random.set_seed(42)\n","model_ann = tf.keras.models.Sequential()\n","n_cols = dummies.shape[1] - 1\n","model_ann.add(tf.keras.layers.Dense(8, input_shape=(n_cols,), activation='relu'))\n","model_ann.add(tf.keras.layers.Dense(1, activation= \"linear\"))\n","model_ann.compile(optimizer = \"adam\", loss = \"mean_squared_error\")\n"]},{"cell_type":"code","execution_count":null,"id":"f99dd805","metadata":{"id":"f99dd805"},"outputs":[],"source":["modelList = {\"Linear Regression\" : model_lr , \"Polynomial Kerner Ridge Regression deg 5\" : model_krr, \"XGboost 20 trees\" : model_xgb, \"Artificial Neural Network\": model_ann}\n"]},{"cell_type":"code","execution_count":null,"id":"e8cdcb73","metadata":{"tags":[],"id":"e8cdcb73"},"outputs":[],"source":["for key, value in modelList.items():\n","    simplePredict(dummies,data_test_idx, value, key)"]},{"cell_type":"markdown","id":"4b3cfae9","metadata":{"id":"4b3cfae9"},"source":["### PCA reduced data"]},{"cell_type":"code","execution_count":null,"id":"1fb4dda1","metadata":{"id":"1fb4dda1"},"outputs":[],"source":["data_num_reduced_df"]},{"cell_type":"code","execution_count":null,"id":"4714957e","metadata":{"id":"4714957e"},"outputs":[],"source":["def PCAPredict(data_num_reduced_df, data_test_idx, model, tag):\n","\n","    # PCA\n","    data_test_PCA = data_num_reduced_df.iloc[data_test_idx]\n","    data_train_PCA = data_num_reduced_df.drop(data_test_idx)\n","\n","\n","    x_train_scaled = data_train_PCA.drop(columns = [\"DeltaHeight\"])\n","    y_train = data_train_PCA[[\"DeltaHeight\"]]\n","\n","    x_test_scaled = data_test_PCA.drop(columns =  [\"DeltaHeight\"])\n","    y_test = data_test_PCA[[\"DeltaHeight\"]]\n","\n","\n","    print(x_train_scaled.shape)\n","    print(y_train.shape)\n","\n","    print(x_test_scaled.shape)\n","    print(y_test.shape)\n","\n","    #TRAIN MODEL\n","    from sklearn.metrics import mean_absolute_error\n","    np.random.seed(2)\n","\n","    print(\"now training\" + key)\n","\n","    if key == \"Artificial Neural Network\":\n","        history = model.fit(x_train_scaled,y_train,epochs=400, validation_split=0.1, verbose = 0)\n","\n","    else:\n","\n","\n","        model.fit(x_train_scaled,y_train)\n","\n","\n","\n","    y_pred_train = model.predict(x_train_scaled)\n","    y_pred_test = model.predict(x_test_scaled)\n","\n","    mae_train = round(mean_absolute_error(y_train, y_pred_train), 2)\n","    mae_test = round(mean_absolute_error(y_test, y_pred_test), 2)\n","\n","\n","    fig,ax = plt.subplots(figsize = (5,5))\n","    plt.suptitle (\"%s \\n mae train =  %smm \\n mae test = %smm  \" % (tag, mae_train,mae_test), y=1)\n","\n","    ax.plot([0, 1], [0, 1], transform=ax.transAxes, linewidth=1, color='black', ls = \":\", alpha = 0.5)\n","    l1 = ax.scatter(y_pred_train, y_train, c= \"m\", alpha = 0.6, label = \"train\")\n","    l2= ax.scatter(y_pred_test, y_test, c= \"r\", alpha = 0.7, label = \"test\")\n","    ax.legend(handles = [l1, l2])\n","    ax.set_xlabel(\"y prediction (mm)\")\n","    ax.set_ylabel(\"y truth (mm) \")\n","    plt.ylim(20, 45)\n","    plt.xlim(20,45)\n","    plt.savefig(\"PCAData%s.png\" % key)\n","\n","\n","    return model, mae_train, mae_test, y_pred_train, y_pred_test\n"]},{"cell_type":"code","execution_count":null,"id":"3906d4e1","metadata":{"id":"3906d4e1"},"outputs":[],"source":["from sklearn.linear_model import LinearRegression\n","model_lr = LinearRegression()\n","\n","from sklearn.kernel_ridge import KernelRidge\n","model_krr = KernelRidge(alpha=1.0,kernel='polynomial', degree=5)\n","\n","import xgboost as xgb\n","model_xgb = xgb.XGBRegressor(max_depth = 4, n_estimaros = 20)\n","\n","import tensorflow as tf\n","tf.random.set_seed(42)\n","model_ann = tf.keras.models.Sequential()\n","n_cols = data_num_reduced_df.shape[1] - 1\n","model_ann.add(tf.keras.layers.Dense(4, input_shape=(n_cols,), activation='relu'))\n","model_ann.add(tf.keras.layers.Dense(3, activation='relu'))\n","model_ann.add(tf.keras.layers.Dense(1, activation= \"linear\"))\n","model_ann.compile(optimizer = \"adam\", loss = \"mean_squared_error\")\n","\n","modelList = {\"Linear Regression\" : model_lr , \"Polynomial Kerner Ridge Regression deg 5 \" : model_krr, \"XGboost 20 trees\" : model_xgb, \"Artificial Neural Network\": model_ann}\n","\n"]},{"cell_type":"code","execution_count":null,"id":"73fb7b54","metadata":{"id":"73fb7b54"},"outputs":[],"source":["for key, value in modelList.items():\n","    PCAPredict(data_num_reduced_df, data_test_idx, value, key)"]},{"cell_type":"markdown","id":"07fdfe3c","metadata":{"id":"07fdfe3c"},"source":["## AUGMENTED DATA"]},{"cell_type":"code","execution_count":null,"id":"93dbc418","metadata":{"id":"93dbc418"},"outputs":[],"source":["augmented = augmented.drop(columns = [\"EdgesNum\", \"clusterPCA\", \"BlockID\",\"origin\",\"Cross?\", \"maleJ\", \"femaleJ\"])\n","augmented"]},{"cell_type":"code","execution_count":null,"id":"e2501318","metadata":{"id":"e2501318"},"outputs":[],"source":["#NO SCALING OF FINAL VARIABLE\n","def augmentPredict(dummies,augmented, data_test_idx, model, times, instructionsDic, catcols, tag):\n","    #compose the dataset because augmented is just training set, you need to extract test set from dummies\n","\n","\n","    common_columns = dummies.columns.intersection(augmented.columns)\n","    dumdum = dummies[common_columns]\n","\n","    #train scaler for original dataset \"dummies\" with 9 features\n","    dummies_x = dumdum.drop(columns = [\"DeltaHeight\"])\n","\n","    from sklearn.preprocessing import StandardScaler\n","    scalerAug = StandardScaler()\n","\n","    #scale test samples\n","    dummies_scaled = scalerAug.fit_transform(dummies_x)\n","    dummies_scaled_df = pd.DataFrame(dummies_scaled, columns=dummies_x.columns)\n","    dummies_scaled_df[\"DeltaHeight\"] = dummies[\"DeltaHeight\"]\n","\n","\n","    filename = 'stdScaler_8features.pk'\n","    pickle.dump(scaler, open(filename, 'wb'))\n","\n","\n","    aug_x = augmented.drop(columns = [\"DeltaHeight\"])\n","\n","    #scale train samples\n","    aug_scaled = scalerAug.transform(aug_x)\n","    aug_scaled_df = pd.DataFrame(aug_scaled, columns=aug_x.columns)\n","    aug_scaled_df[\"DeltaHeight\"] = augmented[\"DeltaHeight\"]\n","\n","    #cast data\n","    data_train = aug_scaled_df\n","    data_test = dummies_scaled_df.iloc[data_test_idx]\n","\n","\n","    x_train_scaled = data_train.drop(columns = [\"DeltaHeight\"])\n","    y_train = data_train[[\"DeltaHeight\"]]\n","\n","    x_test_scaled = data_test.drop(columns = [\"DeltaHeight\"])\n","    y_test = data_test[[\"DeltaHeight\"]]\n","\n","\n","\n","\n","    #TRAIN MODEL\n","    from sklearn.metrics import mean_absolute_error\n","    np.random.seed(2)\n","\n","    print(\"now training\" + key)\n","    if tag == \"Artificial Neural Network\":\n","        history = model.fit(x_train_scaled,y_train,epochs=400, validation_split=0.1, verbose = 0)\n","\n","    else:\n","\n","\n","        model.fit(x_train_scaled,y_train)\n","\n","\n","#     train_score = round(model.score(x_train_scaled, y_train),2)\n","#     test_score = round(model.score(x_test_scaled, y_test), 2)\n","\n","    y_pred_train = model.predict(x_train_scaled)\n","    y_pred_test = model.predict(x_test_scaled)\n","\n","    mae_train = round(mean_absolute_error(y_train, y_pred_train), 2)\n","    mae_test = round(mean_absolute_error(y_test, y_pred_test), 2)\n","\n","\n","    fig,ax = plt.subplots(figsize = (5,5))\n","    plt.suptitle (\"%s \\n mae train =  %smm \\n mae test = %smm  \" % (tag, mae_train,mae_test), y=1)\n","\n","    ax.plot([0, 1], [0, 1], transform=ax.transAxes, linewidth=1, color='black', ls = \":\", alpha = 0.5)\n","    l1 = ax.scatter(y_pred_train, y_train, c= \"m\", alpha = 0.6, label = \"train\")\n","    l2= ax.scatter(y_pred_test, y_test, c= \"r\", alpha = 0.7, label = \"test\")\n","    ax.legend(handles = [l1, l2])\n","    ax.set_xlabel(\"y prediction (mm)\")\n","    ax.set_ylabel(\"y truth (mm) \")\n","    plt.ylim(20, 45)\n","    plt.xlim(20,45)\n","    plt.savefig(\"GAData%s.png\" % tag)\n","\n","\n","    return model, mae_train, mae_test, y_pred_train, y_pred_test\n",""]},{"cell_type":"code","execution_count":null,"id":"bec4e385","metadata":{"id":"bec4e385"},"outputs":[],"source":["from sklearn.linear_model import LinearRegression\n","model_lr = LinearRegression()\n","\n","from sklearn.kernel_ridge import KernelRidge\n","model_krr = KernelRidge(alpha=1.0,kernel='polynomial',degree=5)\n","\n","import xgboost as xgb\n","model_xgb = xgb.XGBRegressor(max_depth = 4, n_estimators = 20)\n","\n","import tensorflow as tf\n","tf.random.set_seed(42)\n","model_ann = tf.keras.models.Sequential()\n","n_cols = augmented.shape[1] - 1\n","model_ann.add(tf.keras.layers.Dense(4, input_shape=(n_cols,), activation='relu'))\n","model_ann.add(tf.keras.layers.Dense(1, activation= \"linear\"))\n","model_ann.compile(optimizer = \"adam\", loss = \"mean_squared_error\")\n","\n","modelList = {\"Linear Regression\" : model_lr , \"Polynomial Kerner Ridge Regression deg 5\" : model_krr, \"XGboost 20 trees\" : model_xgb, \"Artificial Neural Network\": model_ann}\n","\n","times = 5"]},{"cell_type":"code","execution_count":null,"id":"e36a4aa7","metadata":{"id":"e36a4aa7"},"outputs":[],"source":["for key, value in modelList.items():\n","    augmentPredict(dummies, augmented, data_test_idx, value, times, instructionsDic, catCols, key)"]},{"cell_type":"markdown","id":"3e20012d","metadata":{"id":"3e20012d"},"source":["## Additional experiments"]},{"cell_type":"markdown","id":"83caa9ba","metadata":{"id":"83caa9ba"},"source":["### TIGNA on whole dataset"]},{"cell_type":"code","execution_count":null,"id":"c3c9f5e1","metadata":{"id":"c3c9f5e1"},"outputs":[],"source":["def augmentPredict_WHOLE(data, model, times, instructionsDic, catcols, tag):\n","    if times > 1:\n","\n","        #########CREATE AUGMENTED DATA\n","        #init\n","        noisy = pd.DataFrame()\n","        #print(noisy)\n","\n","        #Numerical values\n","        for key, value in instructionsDic.items():\n","            noisy[key] = createNoise(data, key, value[0], value[1], value[2], value[3], times)\n","\n","        #CategoricalValues\n","        duplicateCats = createDuplicate(data, catCols,times)\n","\n","        #Merge\n","        augmented = noisy.join(duplicateCats)\n","        augmented = augmented[:-1]\n","        #print(augmented)\n","\n","        #Look at distributions\n","        for item in noisy.columns:\n","            plt.figure(item)\n","            newDF = pd.DataFrame()\n","            newDF[\"augmented\"] = augmented[item]\n","            newDF = newDF.join(data[item])\n","            #print(item)\n","            #print(newDF.describe().transpose())\n","            #sns.kdeplot(data = newDF, common_norm = False).set(title = item)\n","            plt.show()\n","\n","        #create new column to see if it is real or synthetic\n","        augmented[\"origin\"] = \"real\"\n","        augmented[\"origin\"][augmented.index > 88] = \"synthetic\"\n","\n","\n","        data = augmented\n","\n","    else:\n","        data[\"origin\"] = \"real\"\n","\n","\n","    ###########RUN PCA ON NUMERICAL DATA\n","    data_num = data[['Offset', 'PrintHeight', 'WetLength', 'WetAngStart', 'WetArea','DeltaHeight']]\n","\n","    from sklearn.preprocessing import StandardScaler\n","    scaler = StandardScaler()\n","    data_num_scaled = scaler.fit_transform(data_num)\n","    #print(data_ingredients_scaled)\n","\n","    from sklearn.decomposition import PCA\n","    model_pca = PCA()\n","    model_pca.fit(data_num_scaled)\n","    data_num_reduced = model_pca.transform(data_num_scaled)\n","    data_num_reduced_df = pd.DataFrame(data_num_reduced)\n","    data_cat = data[['FemMal?', 'Cross?', 'BlockID', 'origin']]\n","    data_num_reduced_df= data_num_reduced_df.join(data_cat)\n","\n","    data_num_reduced_df_rev = data_num_reduced_df.reindex(index=data_num_reduced_df.index[::-1])\n","    data_num_reduced_df = data_num_reduced_df.drop(columns = [\"BlockID\", \"Cross?\", \"FemMal?\", \"origin\"])\n","\n","    ##########CLUSTER PCA DATA\n","    from sklearn.cluster import KMeans\n","    kmeans_pca, clustered_PCAdata = clusterPlot(8, data_num_reduced_df, 5, \"PCA-reduced\", False)\n","    data[\"clusterPCA\"] = clustered_PCAdata[\"cluster\"]\n","    #sns.catplot(kind = \"count\", x = \"clusterPCA\", data = data)\n","\n","    ##########PREPARE DATA\n","    #make joint type dummy data and rename columns\n","    dummies = pd.get_dummies(data = data, columns = [\"FemMal?\"], drop_first = True)\n","    colnames = dummies.columns.values.tolist()\n","    colnames = colnames[:-2] + [\"maleJ\", \"femaleJ\"]\n","    dummies.columns = colnames\n","    #dummies[\"Cross?\"] = dummies[\"Cross?\"].map({\"no\": 0, \"yes\": 1})\n","    dummies_x = dummies.drop(columns = [\"BlockID\", \"origin\", \"clusterPCA\", \"DeltaHeight\", \"edgeID\"])\n","    #print(dummies_x.head(5))\n","    #dummies now has the numerical values that need to be standard scaled\n","\n","    from sklearn.preprocessing import StandardScaler\n","    scaler = StandardScaler()\n","    dummies_scaled = scaler.fit_transform(dummies_x)\n","    dummies_scaled_df = pd.DataFrame(dummies_scaled, columns=dummies_x.columns)\n","    dummies_scaled_df[\"DeltaHeight\"] = dummies[\"DeltaHeight\"]\n","\n","    ############SPLIT TRAIN AND TEST\n","    spg = max(2, int(min(list(data.groupby(\"clusterPCA\").size())) / 8))\n","    data_test_idx = dummies.groupby(\"clusterPCA\").sample(n=8, random_state=40).index   # IMPORTANT SET RANDOOM STATE\n","    #print(\" %s test samples\" % len(data_test_idx), data_test_idx)\n","\n","    data_test = dummies_scaled_df.iloc[data_test_idx]\n","    data_train = dummies_scaled_df.drop(data_test_idx)\n","    #print(data_test.info(), data_train.info())\n","\n","    x_train_scaled = data_train.drop(columns = [\"DeltaHeight\"])\n","    y_train = data_train[[\"DeltaHeight\"]]\n","\n","    x_test_scaled = data_test.drop(columns = [\"DeltaHeight\"])\n","    y_test = data_test[[\"DeltaHeight\"]]\n","\n","\n","    #TRAIN MODEL\n","    from sklearn.metrics import mean_absolute_error\n","    np.random.seed(2)\n","\n","\n","    if tag == \"Artificial Neural Network\":\n","        history = model.fit(x_train_scaled,y_train,epochs=400, validation_split=0.1, verbose = 0)\n","\n","    else:\n","\n","\n","        model.fit(x_train_scaled,y_train)\n","\n","\n","#     train_score = round(model.score(x_train_scaled, y_train),2)\n","#     test_score = round(model.score(x_test_scaled, y_test), 2)\n","\n","    y_pred_train = model.predict(x_train_scaled)\n","    y_pred_test = model.predict(x_test_scaled)\n","\n","    mae_train = round(mean_absolute_error(y_train, y_pred_train), 2)\n","    mae_test = round(mean_absolute_error(y_test, y_pred_test), 2)\n","\n","\n","    fig,ax = plt.subplots(figsize = (5,5))\n","    plt.suptitle (\"%s \\n mae train =  %smm \\n mae test = %smm  \" % (tag, mae_train,mae_test), y=1)\n","\n","    ax.plot([0, 1], [0, 1], transform=ax.transAxes, linewidth=1, color='black', ls = \":\", alpha = 0.5)\n","    l1 = ax.scatter(y_pred_train, y_train, c= \"m\", alpha = 0.6, label = \"train\")\n","    l2= ax.scatter(y_pred_test, y_test, c= \"r\", alpha = 0.7, label = \"test\")\n","    ax.legend(handles = [l1, l2])\n","    ax.set_xlabel(\"y prediction (mm)\")\n","    ax.set_ylabel(\"y truth (mm) \")\n","    plt.ylim(20, 45)\n","    plt.xlim(20,45)\n","    plt.savefig(\"WD_GAData%s.png\" % tag)\n","\n","\n","    return model, mae_train, mae_test, y_pred_train, y_pred_test\n",""]},{"cell_type":"code","execution_count":null,"id":"bdb5b776","metadata":{"id":"bdb5b776"},"outputs":[],"source":["instructionsDic = {\"Offset\" : [0, 1, -2, +2], \"PrintHeight\": [0,1, -2, 2], \"WetLength\" : [0,0.5, -1, 1], \"WetAngStart\" : [0, 0.125, -0.25, 0.25], \"WetArea\" : [0, 0.0005, -0.001, 0.001], \"DeltaHeight\" : [0,0.5,-1,1]}\n","catCols = [\"FemMal?\", \"Cross?\", \"BlockID\", \"edgeID\"]\n","\n","from sklearn.linear_model import LinearRegression\n","model_lr = LinearRegression()\n","\n","from sklearn.kernel_ridge import KernelRidge\n","model_krr = KernelRidge(alpha=1.0,kernel='polynomial',degree=5)  #check this\n","\n","import xgboost as xgb\n","model_xgb = xgb.XGBRegressor(max_depth = 4, n_estimators = 20)\n","\n","import tensorflow as tf\n","tf.random.set_seed(42)\n","model_ann = tf.keras.models.Sequential()\n","n_cols = 8\n","model_ann.add(tf.keras.layers.Dense(4, input_shape=(n_cols,), activation='relu'))\n","model_ann.add(tf.keras.layers.Dense(1, activation= \"linear\"))\n","model_ann.compile(optimizer = \"adam\", loss = \"mean_squared_error\")\n","\n","modelList = {\"Linear Regression\" : model_lr , \"Polynomial Kerner Ridge Regression deg 5\" : model_krr, \"XGboost 20 trees\" : model_xgb, \"Artificial Neural Network\": model_ann}\n","\n","times = 5"]},{"cell_type":"code","execution_count":null,"id":"4f0b2bb4","metadata":{"id":"4f0b2bb4"},"outputs":[],"source":["data_v = pd.read_csv('DemonstratorDataset.csv', delim_whitespace= True)\n","data_v['pos'] = data_v['edgeID'].str.find(';')\n","data_v['BlockID'] = data_v.apply(lambda x: x['edgeID'][0:x['pos']],axis=1)\n","data_v[\"BlockID\"] = data_v[\"BlockID\"].str[1:]\n","data_v[\"WetLength\"] = data_v[\"WetLength\"] * 10\n","data_v[\"DryLength\"] = data_v[\"DryLength\"] * 10\n","data_v = data_v.drop('pos', 1)\n","#print(data_v)\n","for key, value in modelList.items():\n","    augmentPredict_WHOLE(data_v, value, times, instructionsDic, catCols, key)"]},{"cell_type":"markdown","id":"c8c1d931","metadata":{"id":"c8c1d931"},"source":["### Random train test split on raw data"]},{"cell_type":"code","execution_count":null,"id":"02c12b54","metadata":{"tags":[],"id":"02c12b54"},"outputs":[],"source":["def RDPredict(dummies, model, tag, seed):\n","\n","    dummies_x = dummies.drop(columns = [\"DeltaHeight\"])\n","\n","    from sklearn.preprocessing import StandardScaler\n","    scaler = StandardScaler()\n","    dummies_scaled = scaler.fit_transform(dummies_x)\n","\n","    dummies_scaled_df = pd.DataFrame(dummies_scaled, columns=dummies_x.columns)\n","    rv = np.asarray(dummies[\"DeltaHeight\"]).reshape(-1,1)\n","\n","    from sklearn.model_selection import train_test_split\n","    x_train_scaled, x_test_scaled, y_train, y_test = train_test_split(dummies_scaled_df, rv, test_size=0.20, random_state=seed)\n","\n","    #TRAIN MODEL\n","    from sklearn.metrics import mean_absolute_error\n","    np.random.seed(2)\n","\n","    #print(\"now training \" + str(seed) )\n","\n","    model.fit(x_train_scaled,y_train)\n","\n","    y_pred_train = model.predict(x_train_scaled)\n","    y_pred_test = model.predict(x_test_scaled)\n","\n","    mae_train = round(mean_absolute_error(y_train, y_pred_train), 2)\n","    mae_test = round(mean_absolute_error(y_test, y_pred_test), 2)\n","\n","    return model, mae_train, mae_test, y_pred_train, y_pred_test\n"]},{"cell_type":"code","execution_count":null,"id":"95f0e2d7","metadata":{"id":"95f0e2d7"},"outputs":[],"source":["from sklearn.linear_model import LinearRegression\n","model_lr = LinearRegression()\n","\n","from sklearn.kernel_ridge import KernelRidge\n","model_krr = KernelRidge(alpha=1.0,kernel='polynomial',degree=5)\n","\n","import xgboost as xgb\n","model_xgb = xgb.XGBRegressor(max_depth = 4, n_estimators = 20)\n","\n","modelList = {\"Linear Regression\" : model_lr , \"Polynomial Kerner Ridge Regression deg 5\" : model_krr, \"XGboost 20 trees\" : model_xgb}\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"641adc6c","metadata":{"id":"641adc6c"},"outputs":[],"source":["import random\n","seeds = random.sample(range(0, 100), 100)\n","\n","for key, value in modelList.items():\n","    print(key)\n","    simplePredict(dummies,data_test_idx, value, key)\n","\n","    err_train = []\n","    err_test = []\n","    for s in seeds:\n","            model, mae_train, mae_test, y_pred_train, y_pred_test = RDPredict(dummies, value, key, s)\n","            err_train.append(mae_train)\n","            err_test.append(mae_test)\n","\n","\n","    df_xgb = pd.DataFrame({\"mae_train\": err_train, \"mae_test\": err_test})\n","\n","    figuuu = plt.gcf()\n","    az = df_xgb.plot(color=['red', \"pink\"])\n","    az.set_xlabel('Index')\n","    az.set_ylabel('Error (mm)')\n","    az.set_ylim(0,20)\n","    az.set_title('Generalization error wrt Train/Test split \\n %s' % key)\n","    figuuu.savefig('noise %s.jpg' % tag)\n","\n",""]},{"cell_type":"markdown","id":"08106450","metadata":{"id":"08106450"},"source":["### Export Prediction  15-feature data on XG boost 20 trees"]},{"cell_type":"code","execution_count":null,"id":"32a4a6b9","metadata":{"id":"32a4a6b9"},"outputs":[],"source":["dummies_x = dummies.drop(columns = [\"DeltaHeight\"])\n","\n","from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler()\n","dummies_scaled = scaler.fit_transform(dummies_x)\n","\n","dummies_scaled_df = pd.DataFrame(dummies_scaled, columns=dummies_x.columns)\n","dummies_scaled_df[\"DeltaHeight\"] = dummies[\"DeltaHeight\"]\n","\n","data_test = dummies_scaled_df.iloc[data_test_idx]\n","data_train = dummies_scaled_df.drop(data_test_idx)\n","\n","x_train_scaled = data_train.drop(columns = [\"DeltaHeight\"])\n","y_train = data_train[[\"DeltaHeight\"]]\n","\n","x_test_scaled = data_test.drop(columns = [\"DeltaHeight\"])\n","y_test = data_test[[\"DeltaHeight\"]]\n","\n","\n","print(x_train_scaled.shape)\n","print(y_train.shape)\n","\n","print(x_test_scaled.shape)\n","print(y_test.shape)\n"]},{"cell_type":"code","execution_count":null,"id":"d13ae790","metadata":{"id":"d13ae790"},"outputs":[],"source":["from sklearn.metrics import mean_absolute_error\n","import xgboost as xgb\n","\n","np.random.seed(2)\n","model = xgb.XGBRegressor(max_depth = 4, n_estimators = 20)\n","\n","model.fit(x_train_scaled,y_train)\n","\n","predictions = model.predict(dummies_scaled_df.drop(columns = [\"DeltaHeight\"]))"]},{"cell_type":"code","execution_count":null,"id":"35e07aae","metadata":{"id":"35e07aae"},"outputs":[],"source":["dummies[\"predictions\"] = predictions\n","dummies[\"edgeID\"] = data[\"edgeID\"]\n","dummies.to_csv(\"XGB_20_predictions.csv\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}